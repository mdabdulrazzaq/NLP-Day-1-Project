
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer, WordNetLemmatizer
import spacy

# Download NLTK data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Initialize tools
stop_words = set(stopwords.words('english'))
ps = PorterStemmer()
lemmatizer = WordNetLemmatizer()
nlp = spacy.load("en_core_web_sm")

# Sample text
text = "Natural Language Processing is an exciting field. It's full of challenges!"

print("Original Text:")
print(text)

# 1. Tokenization
tokens = word_tokenize(text)
print("\nTokens:")
print(tokens)

# 2. Stop-word Removal
filtered_tokens = [word for word in tokens if word.lower() not in stop_words]
print("\nFiltered Tokens (No Stopwords):")
print(filtered_tokens)

# 3. Stemming
stemmed_tokens = [ps.stem(word) for word in filtered_tokens]
print("\nStemmed Tokens:")
print(stemmed_tokens)

# 4. Lemmatization
lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]
print("\nLemmatized Tokens:")
print(lemmatized_tokens)

# 5. Named Entity Recognition (NER) using spaCy
doc = nlp(text)
print("\nNamed Entities:")
for entity in doc.ents:
    print(f"{entity.text} ({entity.label_})")
